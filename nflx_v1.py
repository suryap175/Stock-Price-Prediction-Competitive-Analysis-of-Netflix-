# -*- coding: utf-8 -*-
"""NFLX_FinalPresentation_v1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eAbW4OKjwzkg6bwosCS0L9FMAl4kXSQl
"""

!pip install pandas yfinance ta seaborn matplotlib pandas-datareader jinja2 fredapi openpyxl xgboost scikit-learn statsmodels mlflow

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import yfinance as yf
from scipy.optimize import minimize
import ta
import matplotlib.pyplot as plt
import seaborn as sns
from pandas_datareader._utils import RemoteDataError
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import randint
import math
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.ensemble import GradientBoostingRegressor
import xgboost as xgb
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
from hyperopt.pyll import scope

"""# NFLX vs Competitors"""

# Start date
start_date = datetime(2020, 1, 1)
# End date
end_date = datetime(2023, 12, 31)

# Our stock - NFLX
NFLX = yf.download("NFLX", start_date, end_date)

# Competitor stocks
# Apple
AAPL = yf.download("AAPL", start_date, end_date)
# Disney
DIS = yf.download("DIS", start_date, end_date)
# ROKU
ROKU = yf.download("ROKU", start_date, end_date)
# PARA
PARA = yf.download("PARA", start_date, end_date)
# Amazon
AMZN = yf.download("AMZN", start_date, end_date)

cols = list(NFLX.columns)
cols

"""## Subsetting the dataset for the features we need:"""

NFLX_new = NFLX[["Open", "Close", "Adj Close", "Volume"]]
AAPL_new = AAPL[["Open", "Close", "Adj Close", "Volume"]]
DIS_new = DIS[["Open", "Close", "Adj Close", "Volume"]]
ROKU_new = ROKU[["Open", "Close", "Adj Close", "Volume"]]
PARA_new = PARA[["Open", "Close", "Adj Close", "Volume"]]
AMZN_new = AMZN[["Open", "Close", "Adj Close", "Volume"]]

"""## Training period : 01/01/2020 to 12/31/2022"""

# Training period - 2020-22
train_start_date = datetime(2020, 1, 1)
train_end_date = datetime(2023, 5, 31)
# Testing period - 2023
test_start_date = datetime(2023, 6, 1)
test_end_date = datetime(2023, 12, 30)

cols = list(NFLX_new.columns)
for col in cols:
    NFLX_new = NFLX_new.rename(columns={col: "NFLX_" + col})
    AAPL_new = AAPL_new.rename(columns={col: "AAPL_" + col})
    DIS_new = DIS_new.rename(columns={col: "DIS_" + col})
    ROKU_new = ROKU_new.rename(columns={col: "ROKU_" + col})
    PARA_new = PARA_new.rename(columns={col: "PARA_" + col})
    AMZN_new = AMZN_new.rename(columns={col: "AMZN_" + col})

NFLX_new = NFLX_new.loc[train_start_date:train_end_date]
AAPL_new = AAPL_new.loc[train_start_date:train_end_date]
DIS_new = DIS_new.loc[train_start_date:train_end_date]
ROKU_new = ROKU_new.loc[train_start_date:train_end_date]
PARA_new = PARA_new.loc[train_start_date:train_end_date]
AMZN_new = AMZN_new.loc[train_start_date:train_end_date]

final_df = pd.concat([NFLX_new, AAPL_new, DIS_new, ROKU_new, PARA_new, AMZN_new], axis=1)
final_df

final_df.info()

# Report view
report = final_df.describe().T  # Transpose index and columns
report

"""## Kernel Density Plots"""

final_cols = final_df.columns
print(final_cols)

# Plotting 'price' target variable
fig, ax = plt.subplots(24, 1, figsize=(20, 40))
for i, col in enumerate(final_cols):
    sns.histplot(final_df[col], kde=True, bins="auto", ax=ax[i])
    ax[i].set_xlabel(col)
fig.suptitle("Kernel Density Plots", fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.97])
plt.show()

"""## Plotting Adj_Close price for all companies"""

company_list = ["NFLX", "AAPL", "DIS", "ROKU", "PARA", "AMZN"]
plt.figure(figsize=(12, 8))

for company in company_list:
    plt.plot(final_df[company + "_Close"], label=company)

plt.ylabel("Adj Close")
plt.xlabel("Date")
plt.title("Stock Closing Prices")
plt.legend()
plt.show()

"""## Plotting Volume of stocks sold for all companies"""

plt.figure(figsize=(12, 8))
plt.subplots_adjust(top=1.25, bottom=1.2)
plt.title("Total Volume of Stock Traded Each Day")

for company in company_list:
    final_df[company + "_Volume"].plot(label=company)

plt.ylabel("Volume")
plt.xlabel("Date")
plt.legend()
plt.show()

"""## Pairplot to analyze relationship between Close price of all five company stocks"""

# Making a new DataFrame for returns
tech_rets = final_df[
    ["NFLX_Close", "AAPL_Close", "DIS_Close", "ROKU_Close", "PARA_Close", "AMZN_Close"]
].pct_change()
sns.pairplot(tech_rets, kind="reg")

"""## Correlation between Open prices of all five stocks"""

final_df[
    ["NFLX_Open", "AAPL_Open", "DIS_Open", "ROKU_Open", "PARA_Open", "AMZN_Open"]
].corr().style.background_gradient(cmap="coolwarm")

"""## Expected Return vs. Risk"""

rets = tech_rets.dropna()
area = np.pi * 20
plt.figure(figsize=(12, 10))
plt.scatter(rets.mean(), rets.std(), s=area)
plt.xlabel("Expected return")
plt.ylabel("Risk")
for label, x, y in zip(rets.columns, rets.mean(), rets.std()):
    plt.annotate(
        label,
        xy=(x, y),
        xytext=(50, 50),
        textcoords="offset points",
        ha="right",
        va="bottom",
        arrowprops=dict(arrowstyle="-", color="blue", connectionstyle="arc3,rad=-0.3"),
    )

"""## Plotting Stock Trend and Pct. Returns"""

def plot_stock_trend_and_returns(ticker, titles, start_date, end_date, all_returns):
    # Get the data for this ticker
    prices = yf.download(ticker, start_date, end_date)
    prices = prices.Close

    # prices = web.DataReader(ticker, 'yahoo', start=start_date, end=end_date).Close
    prices.index = [d.date() for d in prices.index]

    plt.figure(figsize=(10, 6))

    # Plot stock price
    plt.subplot(2, 1, 1)
    plt.plot(prices)
    plt.title(titles[0], fontsize=16)
    plt.ylabel("Price ($)", fontsize=14)

    # Plot stock returns
    plt.subplot(2, 1, 2)
    plt.plot(all_returns[0], all_returns[1], color="g")
    plt.title(titles[1], fontsize=16)
    plt.ylabel("Pct. Return", fontsize=14)
    plt.axhline(0, color="k", linestyle="--")

    plt.tight_layout()

    plt.show()

"""## Stock Performance Analysis"""

def perform_analysis_for_stock(
    ticker, start_date, end_date, return_period_weeks, verbose=False
):
    """
    Inputs:
        ticker: the ticker symbol to analyze
        start_date: the first date considered in simulation
        end_date: the last date considered in simulation
        return_period_weeks: the number of weeks in which to calculate returns
        verbose: True if you want to print simulation steps

    Outputs:
        average and standard deviation of returns for simulated runs of this ticker within the given date range
    """

    # Get the data for this ticker
    try:
        prices = yf.download(ticker, start_date, end_date)
        prices = prices.Close
    # Could not find data on this ticker
    except (RemoteDataError, KeyError):
        # Return default values
        return -np.inf, np.inf, None

    prices.index = [d.date() for d in prices.index]

    # This will store all simulated returns
    pct_return_after_period = []
    buy_dates = []

    # Assume we buy the stock on each day in the range
    for buy_date, buy_price in prices.items():
        # Get price of the stock after given number of weeks
        sell_date = buy_date + timedelta(weeks=return_period_weeks)

        try:
            sell_price = prices[prices.index == sell_date].iloc[0]
        # Trying to sell on a non-trading day, skip
        except IndexError:
            continue

        # Compute the percent return
        pct_return = (sell_price - buy_price) / buy_price
        pct_return_after_period.append(pct_return)
        buy_dates.append(buy_date)

        if verbose:
            print("Date Buy: %s, Price Buy: %s" % (buy_date, round(buy_price, 2)))
            print("Date Sell: %s, Price Sell: %s" % (sell_date, round(sell_price, 2)))
            print("Return: %s%%" % round(pct_return * 100, 1))
            print("-------------------")

    # If no data collected return default values
    if len(pct_return_after_period) == 0:
        return -np.inf, np.inf, None

    # Report average and deviation of the percent returns
    return (
        np.mean(pct_return_after_period),
        np.std(pct_return_after_period),
        [buy_dates, pct_return_after_period],
    )

# Start date for simulation.
# Further back means more training data but risk of including patterns that no longer exist
# More recent means less training data but only using recent patterns
start_date, end_date = datetime(2023, 6, 1), datetime(2023, 12, 1)

# set number of weeks in which you want to see return
return_period_weeks = 4

# Minimum average return
min_avg_return = 0.1

# Maximum volatility in return
max_dev_return = 0.07

stocks = ["NFLX", "AAPL", "DIS", "ROKU", "PARA", "AMZN"]
for stock in stocks:
    avg_return, dev_return, all_returns = perform_analysis_for_stock(
        stock, start_date, end_date, return_period_weeks
    )
    title_price = "%s\n%s" % (stock, "Common Stock")
    title_return = "Avg Return: %s%% | Dev Return: %s%%" % (
        round(100 * avg_return, 2),
        round(100 * dev_return, 2),
    )
    plot_stock_trend_and_returns(
        stock, [title_price, title_return], start_date, end_date, all_returns
    )

"""# Construction of feature database

**Period - 1st Jan 2020 to 31st December 2023**
"""

# Define dataset start and end date => Two years worth of data
start_date = datetime(2020, 1, 1)
end_date = datetime(2023, 12, 31)

# Downloaded data
NFLX_STOCK = yf.download("NFLX", start_date, end_date)
NFLX_STOCK.describe()

NFLX_STOCK.head()

NFLX_STOCK["Returns"] = NFLX_STOCK["Adj Close"] - NFLX_STOCK["Adj Close"].shift(1)
NFLX_STOCK.head()

"""Calculating daily returns:

Produce the day's difference of the stock dataframe: (np.log(nflx['Open']) - np.log(nflx['Open'].shift(+1)))

When we take the logarithm of the ratio between today's closing price and yesterday's, we're essentially computing the daily percentage change in the stock price. Using logarithms in return calculations helps us handle the additive nature of log-returns, making overall return calculations more interpretable and facilitating mathematical operations.
"""

# Daily return
NFLX_STOCK["Daily_Return"] = np.log(NFLX_STOCK["Adj Close"]) - np.log(NFLX_STOCK["Adj Close"].shift(1))
NFLX_STOCK = NFLX_STOCK.dropna()
NFLX_STOCK.head()

"""
**Feature-set 1: Typical Price, Typical_Price_Return**


`Mid_Price` is the mean value of Open and Close values"""

NFLX_STOCK["Mid_Price"] = NFLX_STOCK[["Open", "Close"]].mean(axis=1)
NFLX_STOCK["Mid_Price_Return"] = (
    np.log(NFLX_STOCK.Mid_Price) - np.log(NFLX_STOCK.Mid_Price.shift(+1))
) * 100.0
NFLX_STOCK = NFLX_STOCK.dropna()
NFLX_STOCK.head()

"""**Feature-set 2: Common Transforms**

* log of price
* pct_change of returns
* difference in returns
* log of 5 day moving average of price
* Daily returns vs. 200 day moving average
* Daily closing price vs. 50 day exponential moving average



"""

NFLX_STOCK["Price_Log"] = np.log(NFLX_STOCK.Mid_Price)
NFLX_STOCK["Returns_Differencing"] = NFLX_STOCK.Returns.diff()
NFLX_STOCK["Returns_Differencing_10"] = NFLX_STOCK.Returns.diff(10)
NFLX_STOCK["Returns_Percent_Change"] = NFLX_STOCK.Returns.pct_change()

# Log of 5 day moving average of volume
NFLX_STOCK["MA_5"] = np.log(NFLX_STOCK.Mid_Price.rolling(5).mean())

# Daily volume vs. 200 day moving average
NFLX_STOCK["Volumne_MA_200"] = (
    NFLX_STOCK.Mid_Price / NFLX_STOCK.Returns.rolling(200).mean() - 1
)

# Daily closing price vs. 50 day Exponential Moving Avg
NFLX_STOCK["Close_EMA_50"] = NFLX_STOCK.Close / NFLX_STOCK.Close.ewm(span=50).mean() - 1

"""**Feature-set 3: Momentum Indicators**

1. AwesomeOscillatorIndicator
"""

NFLX_STOCK['Momentum_AwesomeOscillatorIndicator'] = ta.momentum.AwesomeOscillatorIndicator(NFLX_STOCK.High, NFLX_STOCK.Low,window1 = 5,window2 = 34, fillna=False).awesome_oscillator()

"""2. Kaufman’s Adaptive Moving Average (KAMA)


"""

NFLX_STOCK['Momentum_KAMA'] = ta.momentum.KAMAIndicator(NFLX_STOCK.Close, fillna=False).kama()

"""3. PercentagePriceOscillator"""

NFLX_STOCK['Momentum_PercentagePVolumneOscillator'] = ta.momentum.PercentageVolumeOscillator(NFLX_STOCK.Volume, fillna=False).pvo()

"""4. Rate of Change (ROC)"""

NFLX_STOCK['Momentum_ROC'] = ta.momentum.ROCIndicator(NFLX_STOCK.Close, fillna=False).roc()

"""5. Relative Strength Index (RSI)"""

NFLX_STOCK['Momentum_RSI'] = ta.momentum.RSIIndicator(NFLX_STOCK.Close, fillna=False).rsi()

"""6. Stochastic RSI"""

NFLX_STOCK['Momentum_StochRSIIndicator'] = ta.momentum.StochRSIIndicator(NFLX_STOCK.Close, fillna=False).stochrsi()

"""7. True strength index (TSI)"""

NFLX_STOCK['Momentum_TSIIndicator'] = ta.momentum.TSIIndicator(NFLX_STOCK.Close, fillna=False).tsi()

"""**Feature-set 4: Trend Indicators**

1. Average Directional Movement Index (ADX)
"""

NFLX_STOCK['Trend_ADX'] = ta.trend.ADXIndicator(NFLX_STOCK.High,NFLX_STOCK.Low, NFLX_STOCK.Close, window = 20,fillna=False).adx()

"""2. Aroon Indicator"""

NFLX_STOCK['Trend_AroonIndicator'] = ta.trend.AroonIndicator(NFLX_STOCK.Close, NFLX_STOCK.Low, window=20, fillna=False).aroon_indicator()

"""3. Commodity Channel Index (CCI)

"""

NFLX_STOCK['Trend_CCI'] = ta.trend.CCIIndicator(NFLX_STOCK.High, NFLX_STOCK.Low, NFLX_STOCK.Close, window = 20,fillna=False).cci()

"""4. Detrended Price Oscillator (DPO)"""

NFLX_STOCK['Trend_DPO'] = ta.trend.DPOIndicator(NFLX_STOCK.Close, window = 20, fillna=False).dpo()

"""5. EMA - Exponential Moving Average

"""

NFLX_STOCK['Trend_EMA'] = ta.trend.EMAIndicator(NFLX_STOCK.Close, window = 20, fillna=False).ema_indicator()

"""6. Moving Average Convergence Divergence (MACD)"""

NFLX_STOCK['Trend_MACD'] = ta.trend.MACD(NFLX_STOCK.Close, fillna=False).macd()

"""7. Mass Index (MI)"""

NFLX_STOCK['Trend_MI'] = ta.trend.MassIndex(NFLX_STOCK.High, NFLX_STOCK.Low, fillna=False).mass_index()

"""**Feature-set 5: Volumne Indicator**

1. Chaikin Money Flow (CMF)
"""

NFLX_STOCK['Volumne_CMF'] = ta.volume.ChaikinMoneyFlowIndicator(NFLX_STOCK.High,NFLX_STOCK.Low,NFLX_STOCK.Close, NFLX_STOCK.Volume,window = 20,fillna=False).chaikin_money_flow()

"""2. Ease of movement (EoM, EMV)"""

NFLX_STOCK['Volumne_EOM'] = ta.volume.EaseOfMovementIndicator(NFLX_STOCK.High,NFLX_STOCK.Low, NFLX_STOCK.Volume,window = 20,fillna=False).ease_of_movement()

"""3. Force Index (FI)"""

NFLX_STOCK['Volumne_FI'] = ta.volume.ForceIndexIndicator(NFLX_STOCK.Close, NFLX_STOCK.Volume,window = 20,fillna=False).force_index()

"""4. Money Flow Index (MFI)"""

NFLX_STOCK['Volumne_MFI'] = ta.volume.money_flow_index(NFLX_STOCK.High, NFLX_STOCK.Low, NFLX_STOCK.Close,NFLX_STOCK.Volume, window=20, fillna=False)

"""5. Volume Weighted Average Price (VWAP)"""

NFLX_STOCK['Volumne_VWAP'] = ta.volume.VolumeWeightedAveragePrice(NFLX_STOCK.High, NFLX_STOCK.Low, NFLX_STOCK.Close,NFLX_STOCK.Volume, window=20, fillna=False).volume_weighted_average_price()

"""**Feature-set 6: Volatility Indicators**

1. Average True Range (ATR)
"""

NFLX_STOCK['Volatility_ATR'] = ta.volatility.AverageTrueRange(NFLX_STOCK.High, NFLX_STOCK.Low, NFLX_STOCK.Close, window=20, fillna=False).average_true_range()

"""2. Bollinger Bands"""

NFLX_STOCK['Volatility_BB'] = ta.volatility.BollingerBands(NFLX_STOCK.Close, window=20, fillna=False).bollinger_wband()

"""3. Donchian Channel"""

NFLX_STOCK['Volatility_DonchainChannel'] = ta.volatility.DonchianChannel(NFLX_STOCK.High, NFLX_STOCK.Low,NFLX_STOCK.Close, window=20, fillna=False).donchian_channel_wband()

"""4. Ulcer Index"""

NFLX_STOCK['Volatility_UlcerIndex'] = ta.volatility.UlcerIndex(NFLX_STOCK.Close, window=20, fillna=False).ulcer_index()

"""5. Keltner channel (KC)"""

NFLX_STOCK['Volatility_KeltnerChannel'] = ta.volatility.keltner_channel_hband(NFLX_STOCK.High, NFLX_STOCK.Low,NFLX_STOCK.Close, window=20, fillna=False)

NFLX_STOCK.dropna(inplace = True)

"""****Feature-set 7: Fama-French 5 Factors Indicators****"""

df_fama = pd.read_csv("/content/F-F_Research_Data_5_Factors_2x3_daily.CSV", skiprows=3)
df_fama = df_fama.iloc[:-1]
df_fama.rename(columns={"Unnamed: 0": "Date"}, inplace=True)
df_fama['Date'] = pd.to_datetime(df_fama['Date'], format='%Y%m%d')
df_fama = df_fama[(df_fama["Date"] >= start_date) & (df_fama["Date"] <= end_date)]
fama = df_fama.set_index("Date")

fama.info()

NFLX_STOCK = pd.concat([NFLX_STOCK, fama], axis=1)
NFLX_STOCK.dropna(inplace=True)
NFLX_STOCK

"""**Feature-set 8: Exracting external factors using Fred API**"""

from fredapi import Fred
key = "605fa38bc581dd058d0a16c59ef616d7"
fred = Fred(api_key=key)

# UK Pound to U.S. Dollar Spot Exchange Rate
# U.S. Dollars to Euro Spot Exchange Rate
# Corporate bond yield
# Coinbase Bitcoin

feat_list = ["SP500","DEXUSUK", "DEXUSEU","DAAA" ,"CBBTCUSD"]
feat_df = pd.DataFrame()
for feat in feat_list:
    feature = fred.get_series(feat, start_date, end_date)
    feature = feature.to_frame(feat)
    feature.dropna(inplace=True)
    feat_df = pd.concat([feat_df, feature], axis=1)
feat_df.dropna(inplace=True)
feat_df

NFLX_STOCK = pd.concat([NFLX_STOCK, feat_df], axis=1)
NFLX_STOCK.dropna(inplace=True)
NFLX_STOCK

"""**Feature-set 8: ADS features**"""

ads = pd.read_excel("/content/ADS_Index_Most_Current_Vintage.xlsx")
ads.rename(columns={"Unnamed: 0": "Date"}, inplace=True)
ads["Date"] = pd.to_datetime(ads["Date"], format="%Y:%m:%d")
ads = ads[(ads["Date"] >= start_date) & (ads["Date"] <= end_date)]
ads = ads.set_index("Date")
ads

ads.info()

"""**Final Dataframe "NFLX_STOCK" is created by gathering all the required data from various features**"""

NFLX_STOCK = pd.concat([NFLX_STOCK, ads], axis=1)
NFLX_STOCK.dropna(inplace=True)
NFLX_STOCK

nflx = NFLX_STOCK
nflx.info()

"""# Virtualize the feature importance & feature selection process using regression & decision tree"""

nflx = pd.read_csv("/content/NFLX_STOCK_WithFeatures.csv")

nflx.set_index("Unnamed: 0", inplace=True)

nflx.index = pd.to_datetime(nflx.index)
nflx = nflx[nflx.index >= pd.to_datetime("2020-01-01")]
split_date = pd.to_datetime("2023-05-31")

df_train = nflx[nflx.index <= split_date]
df_test = nflx[nflx.index > split_date]

print(f"Number of rows in df_train: {len(df_train)}")
print(f"Number of rows in df_test: {len(df_test)}")

columns_to_keep = []
columns_to_drop = ["Open", "High", "Low", "Typical_Price", "Adj Close"]

X_train = df_train
X_train = X_train.drop("Close", axis=1)
y_train = df_train["Close"].shift(-1)[:-1]

X_test = df_test
X_test = X_test.drop("Close", axis=1)
y_test = df_test["Close"].shift(-1)[:-1]

# Dropping the last NaN value from y_train and y_test which resulted from the shift operation
y_train = y_train.dropna()
y_test = y_test.dropna()

# Adjusting X_train and X_test to match the length of y_train and y_test
X_train = X_train.iloc[:len(y_train)]
X_test = X_test.iloc[:len(y_test)]

# Training the Ridge regression model
ridge_model = Ridge(alpha=1.0, fit_intercept=False)
ridge_model.fit(X_train, y_train)

# Predicting on the test set
y_pred = ridge_model.predict(X_test)

# Evaluating the model's performance
mse_ridge = mean_squared_error(y_test, y_pred)
rmse_ridge = np.sqrt(mse_ridge)
r2_ridge = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse_ridge)
print("Root Mean Squared Error:", rmse_ridge)
print("R² Score:", r2_ridge)

# Visualizing the feature importance
feature_importance = np.abs(ridge_model.coef_)
indices = np.argsort(np.abs(feature_importance))[::-1]  # Sort by magnitude
plt.figure(figsize=(10, 8))
plt.bar(range(X_train.shape[1]), feature_importance[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.title('Feature Importance in Ridge Regression Model')
plt.xlabel('Feature')
plt.ylabel('Coefficient Magnitude')
plt.tight_layout()
plt.show()

"""DEXUSEU

Close_EMA_50

Momentum_StochRSIIndicator
"""

# Setting up the Lasso model. Adjust 'alpha' as needed:
# - 'alpha' controls the strength of the regularization.
lasso = Lasso(alpha=1.0, random_state=42)

# Training the Lasso model
lasso.fit(X_train, y_train)

# Predicting on the test set
y_pred = lasso.predict(X_test)

# Evaluating the model's performance
lasso_mse = mean_squared_error(y_test, y_pred)
lasso_rmse = np.sqrt(lasso_mse)
lasso_r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", lasso_mse)
print("Root Mean Squared Error (RMSE):", lasso_rmse)
print("R² Score:", lasso_r2)

# Visualizing the feature importance
feature_importance = np.abs(lasso.coef_)
indices = np.argsort(np.abs(feature_importance))[::-1]  # Sort by magnitude
plt.figure(figsize=(10, 8))
plt.bar(range(X_train.shape[1]), feature_importance[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.title('Feature Importance in Lasso Model')
plt.xlabel('Feature')
plt.ylabel('Coefficient Magnitude')
plt.tight_layout()
plt.show()

"""Open

Adj Close

High

Momentum_KAMA

Mkt-RF

Momentum_AwesomeOscillatorIndicator
"""

elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)

# Training the Elastic Net model
elastic_net.fit(X_train, y_train)

# Predicting on the test set
y_pred = elastic_net.predict(X_test)

# Evaluating the model's performance
elastic_net_mse = mean_squared_error(y_test, y_pred)
elastic_net_rmse = np.sqrt(elastic_net_mse)
elastic_net_r2 = r2_score(y_test, y_pred)

# print("Mean Squared Error:", mse)
print("Root Mean Squared Error (RMSE):", elastic_net_rmse)

# Visualizing the feature importance
feature_importance = np.abs(elastic_net.coef_)
indices = np.argsort(np.abs(feature_importance))[::-1]  # Sort by magnitude
plt.figure(figsize=(10, 8))
plt.bar(range(X_train.shape[1]), feature_importance[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.title('Feature Importance in Elastic Net Model')
plt.xlabel('Feature')
plt.ylabel('Coefficient Magnitude')
plt.tight_layout()
plt.show()

"""Mkt-RF

Open

Typical_Price_Return

Adj Close

Momentum_AwesomeOscillatorIndicator

High

Volatility_ATR

Momentum_KAMA

Momentum_RSI

Returns
"""

# Setting up the Random Forest Regressor
random_forest = RandomForestRegressor(n_estimators=100, random_state=42)

# Training the Random Forest model
random_forest.fit(X_train, y_train)

# Predicting on the test set
y_pred = random_forest.predict(X_test)

# Evaluating the model's performance
random_forest_mse = mean_squared_error(y_test, y_pred)
random_forest_rmse = np.sqrt(random_forest_mse)
random_forest_r2 = r2_score(y_test, y_pred)

# print("Mean Squared Error:", mse)
print("Root Mean Squared Error (RMSE):", random_forest_rmse)
print("R² Score:", random_forest_r2)

# Visualizing the feature importance
feature_importances = random_forest.feature_importances_
indices = np.argsort(feature_importances)[::-1]  # Sort by importance
plt.figure(figsize=(10, 8))
plt.bar(range(X_train.shape[1]), feature_importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.title('Feature Importance in Random Forest Model')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()

"""Typical_Price

Adj Close

Price_Log

High
"""

# Setting up the XGBoost Regressor
xgboost_model = XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                                 max_depth = 5, alpha = 10, n_estimators = 100)

# Training the XGBoost model
xgboost_model.fit(X_train, y_train)

# Predicting on the test set
y_pred = xgboost_model.predict(X_test)

# Evaluating the model's performance
xgboost_mse = mean_squared_error(y_test, y_pred)
xgboost_rmse = np.sqrt(xgboost_mse)
xgboost_r2 = r2_score(y_test, y_pred)

# print("Mean Squared Error:", mse)
print("Root Mean Squared Error (RMSE):", xgboost_rmse)
print("R² Score:", xgboost_r2)

# Visualizing the feature importance
feature_importances = xgboost_model.feature_importances_
indices = np.argsort(feature_importances)[::-1]  # Sort by importance
plt.figure(figsize=(10, 8))
plt.bar(range(X_train.shape[1]), feature_importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.title('Feature Importance in XGBoost Model')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()

"""High

Typical_Price

Low

Adj Close

# 6 Model Training and comparision of RMSE between all the models
"""

pd.set_option("display.max_columns", None)
plt.rcParams["figure.figsize"] = (15, 20)

nflx = pd.read_csv("/content/NFLX_STOCK_WithFeatures.csv")

start_date = datetime(2023, 5, 31)
end_date = datetime(2023, 12, 31)

NFLX_STOCK = yf.download("NFLX", start_date, end_date)
# NFLX_STOCK.head()
NFLX_STOCK["Returns"] = NFLX_STOCK["Adj Close"] - NFLX_STOCK["Adj Close"].shift(1)
NFLX_STOCK.head()

nflx.set_index("Unnamed: 0", inplace=True)

nflx.index = pd.to_datetime(nflx.index)
nflx = nflx[nflx.index >= pd.to_datetime("2020-01-01")]
split_date = pd.to_datetime("2023-05-31")

df_train = nflx[nflx.index <= split_date]
df_test = nflx[nflx.index > split_date]

print(f"Number of rows in df_train: {len(df_train)}")
print(f"Number of rows in df_test: {len(df_test)}")

columns_to_keep = []
columns_to_drop = ["Open", "High", "Low", "Typical_Price", "Adj Close"]

X_train = df_train
X_train = X_train.drop("Close", axis=1)
y_train = df_train["Close"].shift(-1)[:-1]

X_test = df_test
X_test = X_test.drop("Close", axis=1)
y_test = df_test["Close"].shift(-1)[:-1]

nflx.info()

Model_Performances = pd.DataFrame(columns=['Model', 'RMSE'])
def add_rmse_score(model_name, rmse_score):
    global Model_Performances
    # Create a new DataFrame for the new row
    new_row = pd.DataFrame({'Model': [model_name], 'RMSE': [rmse_score],'Profit':[trade_txn_df['Total($)'].iloc[-1]-10000]})
    # Use concat to add the new row to the existing DataFrame
    Model_Performances = pd.concat([Model_Performances, new_row], ignore_index=True)

"""## Trading Function"""

def merge_results(y_test, y_pred, NVDA_STOCK):
    y_test = y_test.to_frame(name="Actual")
    y_test = y_test.rename_axis("Date")

    y_pred = pd.Series(y_pred, index=y_test.index)
    y_pred = y_pred.to_frame(name="Predicted")
    y_pred = y_pred.rename_axis("Date")

    results = NVDA_STOCK.merge(y_test, left_index=True, right_index=True, how="inner")
    results = results.merge(y_pred, left_index=True, right_index=True, how="inner")
    results = results[["Actual", "Predicted", "Open", "Close"]]

    return results

def execute_trade(result_df, among):
    signal = 0
    amount = 10000
    Amount = []
    balance = 0
    action = []
    portfolio = 0
    Portfolio = []
    stocks = 0
    Stocks = []
    log_df = []

    for i in range(len(result_df) - 1):
        if result_df["Predicted"][i + 1] > result_df["Actual"][i]:
            if signal == 0:
                action.append("Buy")
                stocks = int(amount / result_df["Actual"][i])
                balance = int(amount % result_df["Actual"][i])
                portfolio = stocks * result_df["Actual"][i]
                signal = 1
                amount = portfolio + balance
                info = {
                    "Date": result_df.index[i],
                    "Stock": result_df["Actual"][i],
                    "Action": action[i],
                    "Portfolio": round(portfolio, 2),
                    "Stocks": stocks,
                    "Balance_init": balance,
                    "Total($)": round(amount, 2),
                }
                log_df.append(info)
                Portfolio.append(round(portfolio, 5))
                Amount.append(round(amount, 0))
                Stocks.append(stocks)
            else:
                action.append("Bought-Holding")
                portfolio = stocks * result_df["Actual"][i]
                amount = portfolio + balance
                info = {
                    "Date": result_df.index[i],
                    "Stock": result_df["Actual"][i],
                    "Action": action[i],
                    "Portfolio": round(portfolio, 2),
                    "Stocks": stocks,
                    "Balance_init": balance,
                    "Total($)": round(amount, 2),
                }
                log_df.append(info)
                Portfolio.append(round(portfolio, 5))
                Amount.append(round(amount, 0))
                Stocks.append(stocks)

        elif result_df["Predicted"][i + 1] < result_df["Actual"][i]:
            if signal == 1:
                action.append("Sell")
                portfolio = stocks * result_df["Actual"][i]

                signal = 0
                stocks = 0
                amount = balance + portfolio
                portfolio = 0
                balance = 0
                info = {
                    "Date": result_df.index[i],
                    "Stock": result_df["Actual"][i],
                    "Action": action[i],
                    "Portfolio": round(portfolio, 2),
                    "Stocks": stocks,
                    "Balance_init": balance,
                    "Total($)": round(amount, 2),
                }
                log_df.append(info)
                Portfolio.append(round(portfolio, 5))
                Amount.append(round(amount, 0))
                Stocks.append(stocks)
            else:
                action.append("Price-Prediction-Already-Lower")
                info = {
                    "Date": result_df.index[i],
                    "Stock": result_df["Actual"][i],
                    "Action": action[i],
                    "Portfolio": round(portfolio, 2),
                    "Stocks": stocks,
                    "Balance_init": balance,
                    "Total($)": round(amount, 2),
                }
                log_df.append(info)
                Portfolio.append(round(portfolio, 5))
                Amount.append(round(amount, 0))
                Stocks.append(stocks)

    Strategy_Result = pd.DataFrame(
        {
            "Actual_Close_Price": result_df.Close[1:],
            "Predicted_Close_Price": result_df.Predicted[1:],
            "Date": result_df.index[1:],
            "Amount($)": Amount,
        }
    )

    tnx_log = pd.DataFrame(log_df)

    return tnx_log, Strategy_Result

"""## Data Preprocessing"""

# Dropping the last NaN value from y_train and y_test which resulted from the shift operation
y_train = y_train.dropna()
y_test = y_test.dropna()

# Adjusting X_train and X_test to match the length of y_train and y_test
X_train = X_train.iloc[:len(y_train)]
X_test = X_test.iloc[:len(y_test)]

"""## XGBoost"""

X_train_xgb = X_train[['High',  'Typical_Price', 'Low', 'Adj Close']]

X_test_xgb = X_test[['High',  'Typical_Price', 'Low', 'Adj Close']]

# Define the hyperparameters to tune
param_distributions = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'min_child_weight': randint(1, 5),
    'learning_rate': uniform(0.01, 0.3),
}

# Number of iterations for RandomizedSearch
n_iter_search = 20

# Initialize the XGBRegressor
xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=xgb_reg, param_distributions=param_distributions,
                                   n_iter=n_iter_search, cv=3, scoring='neg_mean_squared_error', verbose=1, random_state=42)

# Fit RandomizedSearchCV
random_search.fit(X_train_xgb, y_train)

# Best parameters and RMSE
best_params = random_search.best_params_
best_rmse = math.sqrt(-random_search.best_score_)

print(f"Best parameters found: {best_params}")
print(f"Best RMSE from RandomizedSearch: {best_rmse:.2f}")

# Retrain the model on the full training set with the best parameters
best_xgb_reg = XGBRegressor(**best_params, random_state=42)
best_xgb_reg.fit(X_train_xgb, y_train)

# Predict on the test set
y_pred = best_xgb_reg.predict(X_test_xgb)

# Calculate RMSE for the test set
xgb_test_mse = mean_squared_error(y_test, y_pred)
xgb_test_rmse = np.sqrt(xgb_test_mse)

print(f"Test RMSE: {xgb_test_rmse:.2f}")

actual_predict_merge_df = merge_results(y_test, y_pred, NFLX_STOCK)
plt.figure(figsize=(15, 4))
plt.plot(actual_predict_merge_df['Actual'], label="Actual_Closing", linestyle="solid")
plt.plot(
    actual_predict_merge_df['Predicted'],
    label="Predicted_Closing",
    linestyle="dashed",
)
plt.title("XGB Regressor - Closing - Actual vs Predicted")
plt.xlabel("Date")
plt.ylabel("Closing")
plt.legend()
plt.figtext(
    0.20,
    0.85,
    f"RSME = {xgb_test_rmse:.2f}",
    ha="right",
    va="top",
    color="red",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

trade_txn_df, trade_txn_log_df = execute_trade(actual_predict_merge_df, 10000)
trade_txn_df

plt.figure(figsize=(15, 4))
plt.plot(trade_txn_log_df["Amount($)"], label="Closing", linestyle="solid")
plt.title("Trading Signal Result - XGB")
plt.xlabel("Date")
plt.ylabel("Investment")
plt.legend(labels = ['Investment'], loc='lower right')
plt.figtext(
    0.22,
    0.85,
    f"Profit = {trade_txn_df['Total($)'].iloc[-1]-10000:.2f}",
    ha="right",
    va="top",
    color="green",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

add_rmse_score('XG Boost', str(xgb_test_rmse))

Model_Performances.head()

"""## Random Forest"""

# Define the hyperparameters to tune
param_distributions = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'min_samples_leaf': randint(1, 5),
    'min_samples_split': randint(2, 10)
}

# Number of iterations for RandomizedSearch
n_iter_search = 20

# Initialize the RandomForestRegressor
rf_reg = RandomForestRegressor(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=rf_reg, param_distributions=param_distributions,
                                   n_iter=n_iter_search, cv=3, scoring='neg_mean_squared_error', verbose=1, random_state=42)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Best parameters and RMSE
best_params = random_search.best_params_
best_rmse = math.sqrt(-random_search.best_score_)

print(f"Best parameters found: {best_params}")
print(f"Best RMSE from RandomizedSearch: {best_rmse:.2f}")

# Retrain the model on the full training set with the best parameters
best_rf_reg = RandomForestRegressor(**best_params, random_state=42)
best_rf_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = best_rf_reg.predict(X_test)

# Calculate RMSE for the test set
rf_test_mse = mean_squared_error(y_test, y_pred)
rf_test_rmse = np.sqrt(rf_test_mse)

print(f"Test RMSE: {rf_test_rmse:.2f}")

actual_predict_merge_df = merge_results(y_test, y_pred, NFLX_STOCK)
plt.figure(figsize=(15, 4))
plt.plot(actual_predict_merge_df['Actual'], label="Actual_Closing", linestyle="solid")
plt.plot(
    actual_predict_merge_df['Predicted'],
    label="Predicted_Closing",
    linestyle="dashed",
)
plt.title("Random Forest Regressor - Closing - Actual vs Predicted")
plt.xlabel("Date")
plt.ylabel("Closing")
plt.legend()
plt.figtext(
    0.20,
    0.85,
    f"RSME = {rf_test_rmse:.2f}",
    ha="right",
    va="top",
    color="red",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

trade_txn_df, trade_txn_log_df = execute_trade(actual_predict_merge_df, 10000)
trade_txn_df

plt.figure(figsize=(15, 4))
plt.plot(trade_txn_log_df["Amount($)"], label="Closing", linestyle="solid")
plt.title("Trading Signal Result - Random Forest")
plt.xlabel("Date")
plt.ylabel("Investment")
plt.legend(labels = ['Investment'], loc='lower right')
plt.figtext(
    0.22,
    0.85,
    f"Profit = {trade_txn_df['Total($)'].iloc[-1]-10000:.2f}",
    ha="right",
    va="top",
    color="green",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

add_rmse_score('Random Forest Regressor', str(rf_test_rmse))

Model_Performances.head()

"""## Ridge Regression"""

# Define the hyperparameters to tune
param_distributions = {
    'alpha': uniform(0.1, 100)  # Uniform distribution over a range 0.1 to 100
}

# Number of iterations for RandomizedSearch
n_iter_search = 20

# Initialize the Ridge regressor
ridge_reg = Ridge(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=ridge_reg, param_distributions=param_distributions,
                                   n_iter=n_iter_search, cv=3, scoring='neg_mean_squared_error', verbose=1, random_state=42)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Best parameters and RMSE
best_params = random_search.best_params_
best_rmse = np.sqrt(-random_search.best_score_)

print(f"Best parameters found: {best_params}")
print(f"Best RMSE from RandomizedSearch: {best_rmse:.2f}")

# Retrain the model on the full training set with the best parameters
best_ridge_reg = Ridge(**best_params, random_state=42)
best_ridge_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = best_ridge_reg.predict(X_test)

# Calculate RMSE for the test set
ridge_test_mse = mean_squared_error(y_test, y_pred)
ridge_test_rmse = np.sqrt(ridge_test_mse)

print(f"Test RMSE: {ridge_test_rmse:.2f}")

actual_predict_merge_df = merge_results(y_test, y_pred, NFLX_STOCK)
plt.figure(figsize=(15, 4))
plt.plot(actual_predict_merge_df['Actual'], label="Actual_Closing", linestyle="solid")
plt.plot(
    actual_predict_merge_df['Predicted'],
    label="Predicted_Closing",
    linestyle="dashed",
)
plt.title("Ridge - Closing - Actual vs Predicted")
plt.xlabel("Date")
plt.ylabel("Return")
plt.legend()
plt.figtext(
    0.20,
    0.85,
    f"RSME = {ridge_test_rmse:.2f}",
    ha="right",
    va="top",
    color="red",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

trade_txn_df, trade_txn_log_df = execute_trade(actual_predict_merge_df, 10000)
trade_txn_df

plt.figure(figsize=(15, 4))
plt.plot(trade_txn_log_df["Amount($)"], label="Actual_Closing", linestyle="solid")
plt.title("Trading Signal Result - Ridge")
plt.xlabel("Date")
plt.ylabel("Investment")
plt.legend(labels = ['Investment'], loc='lower right')
plt.figtext(
    0.22,
    0.85,
    f"Profit = {trade_txn_df['Total($)'].iloc[-1]-10000:.2f}",
    ha="right",
    va="top",
    color="green",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

add_rmse_score('Ridge Regression', str(ridge_test_rmse))

Model_Performances.head()

"""## Lasso Regression

"""

X_train_lasso = X_train[['Open', 'Adj Close', 'High', 'Momentum_KAMA', 'Mkt-RF', 'Momentum_AwesomeOscillatorIndicator']]
X_test_lasso = X_test[['Open', 'Adj Close', 'High', 'Momentum_KAMA', 'Mkt-RF', 'Momentum_AwesomeOscillatorIndicator']]

# Define the hyperparameters to tune
param_distributions = {
    'alpha': uniform(0.001, 1)  # Uniform distribution over a range from 0.001 to 1
}

# Number of iterations for RandomizedSearch
n_iter_search = 50

# Initialize the Lasso regressor
lasso = Lasso(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=lasso, param_distributions=param_distributions,
                                   n_iter=n_iter_search, cv=5, scoring='neg_mean_squared_error', verbose=1, random_state=42)

# Fit RandomizedSearchCV
random_search.fit(X_train_lasso, y_train)

# Best parameters and RMSE
best_params = random_search.best_params_
best_rmse = np.sqrt(-random_search.best_score_)

print(f"Best parameters found: {best_params}")
print(f"Best RMSE from RandomizedSearch: {best_rmse:.2f}")

# Retrain the model on the full training set with the best parameters
best_lasso = Lasso(**best_params, random_state=42)
best_lasso.fit(X_train_lasso, y_train)

# Predict on the test set
y_pred = best_lasso.predict(X_test_lasso)

# Calculate RMSE for the test set
test_mse = mean_squared_error(y_test, y_pred)
test_rmse = np.sqrt(test_mse)

print(f"Test RMSE: {test_rmse:.2f}")

actual_predict_merge_df = merge_results(y_test, y_pred, NFLX_STOCK)
plt.figure(figsize=(15, 4))
plt.plot(actual_predict_merge_df['Actual'], label="Actual_Closing", linestyle="solid")
plt.plot(
    actual_predict_merge_df['Predicted'],
    label="Predicted_Closing",
    linestyle="dashed",
)
plt.title("Lasso - Closing - Actual vs Predicted")
plt.xlabel("Date")
plt.ylabel("Return")
plt.legend()
plt.figtext(
    0.20,
    0.85,
    f"RSME = {test_rmse:.2f}",
    ha="right",
    va="top",
    color="red",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

trade_txn_df, trade_txn_log_df = execute_trade(actual_predict_merge_df, 10000)
trade_txn_df

plt.figure(figsize=(15, 4))
plt.plot(trade_txn_log_df["Amount($)"], label="Actual_Closing", linestyle="solid")
plt.title("Trading Signal Result - Lasso")
plt.xlabel("Date")
plt.ylabel("Investment")
plt.legend(labels = ['Investment'], loc='lower right')
plt.figtext(
    0.22,
    0.85,
    f"Profit = {trade_txn_df['Total($)'].iloc[-1]-10000:.2f}",
    ha="right",
    va="top",
    color="green",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

add_rmse_score('Lasso Regression', str(test_rmse))

Model_Performances.head()

"""## ElasticNet Regression"""

# Define the hyperparameters to tune
param_distributions = {
    'alpha': uniform(0.1, 10),  # Uniform distribution over a range 0.1 to 10
    'l1_ratio': uniform(0, 1)  # Uniform distribution between 0 and 1 for l1_ratio
}

# Number of iterations for RandomizedSearch
n_iter_search = 50

# Initialize the ElasticNet regressor
elastic_net = ElasticNet(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=elastic_net, param_distributions=param_distributions,
                                   n_iter=n_iter_search, cv=5, scoring='neg_mean_squared_error', verbose=1, random_state=42)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Best parameters and RMSE
best_params = random_search.best_params_
best_rmse = np.sqrt(-random_search.best_score_)

print(f"Best parameters found: {best_params}")
print(f"Best RMSE from RandomizedSearch: {best_rmse:.2f}")

# Retrain the model on the full training set with the best parameters
best_elastic_net = ElasticNet(**best_params, random_state=42)
best_elastic_net.fit(X_train, y_train)

# Predict on the test set
y_pred = best_elastic_net.predict(X_test)

# Calculate RMSE for the test set
test_mse = mean_squared_error(y_test, y_pred)
EN_test_rmse = np.sqrt(test_mse)


add_rmse_score('ElasticNet Regression', str(EN_test_rmse))

print(f"Test RMSE: {EN_test_rmse:.2f}")

actual_predict_merge_df = merge_results(y_test, y_pred, NFLX_STOCK)
plt.figure(figsize=(15, 4))
plt.plot(actual_predict_merge_df['Actual'], label="Actual_Closing", linestyle="solid")
plt.plot(
    actual_predict_merge_df['Predicted'],
    label="Predicted_Closing",
    linestyle="dashed",
)
plt.title("ElasticNet - Closing - Actual vs Predicted")
plt.xlabel("Date")
plt.ylabel("Return")
plt.legend()
plt.figtext(
    0.20,
    0.85,
    f"RSME = {EN_test_rmse:.2f}",
    ha="right",
    va="top",
    color="red",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

trade_txn_df, trade_txn_log_df = execute_trade(actual_predict_merge_df, 10000)
trade_txn_df

plt.figure(figsize=(15, 4))
plt.plot(trade_txn_log_df["Amount($)"], label="Actual_Closing", linestyle="solid")
plt.title("Trading Signal Result - ElasticNet")
plt.xlabel("Date")
plt.ylabel("Investment")
plt.legend(labels = ['Investment'], loc='lower right')
plt.figtext(
    0.22,
    0.85,
    f"Profit = {trade_txn_df['Total($)'].iloc[-1]-10000:.2f}",
    ha="right",
    va="top",
    color="green",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

add_rmse_score('ElasticNet Regression', str(EN_test_rmse))

Model_Performances.head()

"""## Quantile Regression"""

# Add a constant to the model (the intercept term)
X_train_const = sm.add_constant(X_train)
X_test_const = sm.add_constant(X_test)  # also add constant to the test dataset

quantiles = [0.25, 0.5, 0.75]
results = []
models = []

for quantile in quantiles:
    # Fit the Quantile Regression model
    mod = sm.QuantReg(y_train, X_train_const)
    res = mod.fit(q=quantile)
    models.append(res)  # Store the model results for later use

    # Predict on the training set
    y_pred_train = res.predict(X_train_const)

    # Calculate RMSE for the training set
    train_mse = mean_squared_error(y_train, y_pred_train)
    train_rmse = np.sqrt(train_mse)
    results.append(train_rmse)

    print(f"Quantile: {quantile}")
    print(f"Train RMSE: {train_rmse:.2f}")


# Find the model with the lowest RMSE
min_rmse_index = np.argmin(results)
best_model = models[min_rmse_index]
best_quantile = quantiles[min_rmse_index]

# Predict on the test set using the best model
y_pred = best_model.predict(X_test_const)

# Calculate RMSE for the test set
quantile_test_mse = mean_squared_error(y_test, y_pred)
quantile_test_rmse = np.sqrt(quantile_test_mse)

print(f"Best Quantile: {best_quantile}")
print(f"Test RMSE: {quantile_test_rmse:.2f}")

actual_predict_merge_df = merge_results(y_test, y_pred, NFLX_STOCK)
plt.figure(figsize=(15, 4))
plt.plot(actual_predict_merge_df['Actual'], label="Actual_Closing", linestyle="solid")
plt.plot(
    actual_predict_merge_df['Predicted'],
    label="Predicted_Closing",
    linestyle="dashed",
)
plt.title("Quantile Regression - Closing - Actual vs Predicted")
plt.xlabel("Date")
plt.ylabel("Return")
plt.legend()
plt.figtext(
    0.20,
    0.85,
    f"RSME = {quantile_test_rmse:.2f}",
    ha="right",
    va="top",
    color="red",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

trade_txn_df, trade_txn_log_df = execute_trade(actual_predict_merge_df, 10000)
trade_txn_df

plt.figure(figsize=(15, 4))
plt.plot(trade_txn_log_df["Amount($)"], label="Actual_Closing", linestyle="solid")
plt.title("Trading Signal Result - Quantile Regression")
plt.xlabel("Date")
plt.ylabel("Investment")
plt.legend(labels = ['Investment'], loc='lower right')
plt.figtext(
    0.22,
    0.85,
    f"Profit = {trade_txn_df['Total($)'].iloc[-1]-10000:.2f}",
    ha="right",
    va="top",
    color="green",
    weight="bold",
    bbox=dict(facecolor="yellow", edgecolor="black", boxstyle="round,pad=0.5"),
)
plt.show()

add_rmse_score('Quantile Regression', str(quantile_test_rmse))

Model_Performances

"""# Benchmark Study

## Garch
"""

def GARCH(param, *args):
    mu = param[0]
    omega = param[1]
    alpha = param[2]
    beta = param[3]
    T = Y.shape[0]
    GARCH_Dens = np.zeros(T)
    sigma2 = np.zeros(T)
    F = np.zeros(T)
    v = np.zeros(T)
    for t in range(1, T):
        sigma2[t] = omega + alpha * ((Y[t - 1] - mu) ** 2) + beta * (sigma2[t - 1])
        F[t] = Y[t] - mu - np.sqrt(sigma2[t]) * np.random.normal(0, 1, 1)
        v[t] = sigma2[t]
        GARCH_Dens[t] = (
            (1 / 2) * np.log(2 * np.pi)
            + (1 / 2) * np.log(v[t])
            + (1 / 2) * (F[t] / v[t])
        )
        Likelihood = np.sum(GARCH_Dens[1:-1])
        return Likelihood


def GARCH_PROD(params, Y0, T):
    mu = params[0]
    omega = params[1]
    alpha = params[2]
    beta = params[3]
    Y = np.zeros(T)
    sigma2 = np.zeros(T)
    Y[0] = Y0
    sigma2[0] = 0.003
    for t in range(1, T):
        sigma2[t] = omega + alpha * ((Y[t - 1] - mu) ** 2) + beta * (sigma2[t - 1])
        Y[t] = mu + np.sqrt(sigma2[t]) * np.random.normal(0, 1, 1)
    return Y


start_date = datetime(2020, 1, 1)
end_date = datetime(2023, 12, 31)
NFLX = yf.download("NFLX", start_date, end_date)
Y = NFLX['Adj Close'].values
Y = np.diff(np.log(NFLX["Adj Close"].values))
T = Y.size
param0 = np.array([np.mean(Y), np.var(Y) / 4, 0.15, 0.2]) #To adjust RMSE
param_star = minimize(
    GARCH, param0, method="BFGS", options={"xtol": 1e-8, "disp": True}
)
Y_GARCH = GARCH_PROD(param_star.x, Y[0], T)
timevec = np.linspace(1, T, T)
plt.figure(figsize=(15, 4))
plt.legend()
# plt.plot(timevec, Y_GARCH, "r", timevec, Y, "b")
plt.plot(timevec, Y, "b", label='Original Data')  # Blue for original data
plt.plot(timevec, Y_GARCH, "r", label='Generated GARCH Series')  # Red for generated
RMSE = np.sqrt(np.mean((Y_GARCH - Y) ** 2))

plt.xlabel("Time")
plt.ylabel("Log Returns")
plt.title("Comparison of Original Data and Generated GARCH Series")
plt.legend(loc='lower right')
plt.show()
print("RMSE values is:", RMSE)

# Kalman Filter
def kalman_filter(param,*args):
    # initialize params
    Z = param[0]
    T = param[1]
    H = param[2]
    Q = param[3]
    # initialize vector values:
    u_predict,  u_update,  P_predict, P_update, v, F = {},{},{},{},{},{}
    u_update[0] = Y[0]
    u_predict[0] = u_update[0]
    P_update[0] = np.var(Y)/4
    P_predict[0] =  T*P_update[0]*np.transpose(T)+Q
    Likelihood = 0
    for s in range(1, S):
        F[s] = Z*P_predict[s-1]*np.transpose(Z)+H
        v[s]= Y[s-1]-Z*u_predict[s-1]
        u_update[s] = u_predict[s-1]+P_predict[s-1]*np.transpose(Z)*(1/F[s])*v[s]
        u_predict[s] = T*u_update[s]
        P_update[s] = P_predict[s-1]-P_predict[s-1]*np.transpose(Z)*(1/F[s])*Z*P_predict[s-1]
        P_predict[s] = T*P_update[s]*np.transpose(T)+Q
        Likelihood += (1/2)*np.log(2*np.pi)+(1/2)*np.log(abs(F[s]))+(1/2)*np.transpose(v[s])*(1/F[s])*v[s]

    return Likelihood


def kalman_smoother(params, *args):
    # initialize params
    Z = params[0]
    T = params[1]
    H = params[2]
    Q = params[3]
    # initialize vector values:
    u_predict,  u_update,  P_predict, P_update, v, F = {},{},{},{},{},{}
    u_update[0] = Y[0]
    u_predict[0] = u_update[0]
    P_update[0] = np.var(Y)/4
    P_predict[0] =  T*P_update[0]*np.transpose(T)+Q
    for s in range(1, S):
        F[s] = Z*P_predict[s-1]*np.transpose(Z)+H
        v[s]=Y[s-1]-Z*u_predict[s-1]
        u_update[s] = u_predict[s-1]+P_predict[s-1]*np.transpose(Z)*(1/F[s])*v[s]
        u_predict[s] = T*u_update[s]
        P_update[s] = P_predict[s-1]-P_predict[s-1]*np.transpose(Z)*(1/F[s])*Z*P_predict[s-1]
        P_predict[s] = T*P_update[s]*np.transpose(T)+Q

    u_smooth, P_smooth = {}, {}
    u_smooth[S-1] = u_update[S-1]
    P_smooth[S-1] = P_update[S-1]
    for t in range(S-1, 0, -1):
        u_smooth[t-1] = u_update[t] + P_update[t]*np.transpose(T)/P_predict[t]*(u_smooth[t]-T*u_update[s])
        P_smooth[t-1] = P_update[t] + P_update[t]*np.transpose(T)/P_predict[t]*(P_smooth[t]-P_predict[t])/P_predict[t]*T*P_update[t]

    # del u_update[-1]
    smooth_path = u_smooth
    return smooth_path


start_date = datetime(2020, 1, 1)
end_date = datetime(2023, 12, 31)
stock = yf.download("NFLX",start_date ,end_date)
Y = stock['Adj Close'].values
S = Y.shape[0]

param0 = np.array([1.8, 0.97,np.var(Y)/50, np.var(Y)/50]) #To adjust RMSE

results = minimize(kalman_filter, param0, method='BFGS', options={'xtol': 1e-8, 'disp': True})

# Smooth and visualize the estimated path
param_star = results.x
path = kalman_smoother(param_star, Y, S)
Y_kalmanFilter = np.hstack(list(path.values()))
Y_kalmanFilter = Y_kalmanFilter[::-1]

timevec = np.linspace(1, S, S)

# Modify the figure size here
plt.figure(figsize=(8, 4))  # Smaller figure size (Width, Height in inches)
plt.title('Kalman Filter Stock Price Prediction: Netflix')
plt.plot(timevec, Y, "b", label='Original Data')  # Blue for original data
plt.plot(timevec, Y_kalmanFilter, "r", label='Generated Kalman Filter Price')  # Red for generated
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend(loc='upper right')

RMSE = np.sqrt(np.mean((Y_kalmanFilter - Y)**2))
print('RMSE values is: $', RMSE)

plt.show()

